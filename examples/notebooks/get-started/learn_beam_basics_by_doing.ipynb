{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermevaz8/Dados_larga_escala/blob/main/examples/notebooks/get-started/learn_beam_basics_by_doing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one\n",
        "# or more contributor license agreements. See the NOTICE file\n",
        "# distributed with this work for additional information\n",
        "# regarding copyright ownership. The ASF licenses this file\n",
        "# to you under the Apache License, Version 2.0 (the\n",
        "# \"License\"); you may not use this file except in compliance\n",
        "# with the License. You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing,\n",
        "# software distributed under the License is distributed on an\n",
        "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
        "# KIND, either express or implied. See the License for the\n",
        "# specific language governing permissions and limitations\n",
        "# under the License."
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "BJcdwLiyJaGa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V913rQcmLS72"
      },
      "source": [
        "# Welcome to Apache Beam!\n",
        "\n",
        "This notebook will be your introductory guide to Beam's main concepts and its uses. This tutorial **does not** assume any prior Apache Beam knowledge.\n",
        "\n",
        "We'll cover what Beam is, what it does, and a few basic transforms!\n",
        "\n",
        "We aim to give you familiarity with:\n",
        "- Creating a `Pipeline`\n",
        "- Creating a `PCollection`\n",
        "- Performing basic `PTransforms`\n",
        "  - Map\n",
        "  - Filter\n",
        "  - FlatMap\n",
        "  - Combine\n",
        "- Applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lduh-9oXt3P_"
      },
      "source": [
        "## How To Approach This Tutorial\n",
        "\n",
        "This tutorial was designed for someone who likes to **learn by doing**. As such, there will be opportunities for you to practice writing your own code in these notebooks with the answer hidden in a cell below.\n",
        "\n",
        "Codes that require editing will be with an `...` and each cell title will say `Edit This Code`. However, you are free to play around with the other cells if you would like to add something beyond our tutorial.\n",
        "\n",
        "It may be tempting to just copy and paste solutions, but even if you do look at the Answer cells, try typing out the solutions manually. The muscle memory will be very helpful.\n",
        "\n",
        "> Tip: For those who would like to learn concepts more from the ground up, check out the [Tour of Beam](https://tour.beam.apache.org/)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42B-64Lvef3K"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "We'll assume you have familiarity with Python or Pandas, but you should be able to follow along even if you’re coming from a different programming language. We'll also assume you understand programming concepts like functions, objects, arrays, and dictionaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeDD0nardXyL"
      },
      "source": [
        "## Running CoLab\n",
        "\n",
        "To navigate through different sections, use the table of contents. From View drop-down list, select Table of contents.\n",
        "\n",
        "To run a code cell, you can click the Run cell button at the top left of the cell, or by select it and press `Shift+Enter`. Try modifying a code cell and re-running it to see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdMPnMDDkGc8"
      },
      "source": [
        "To begin, we have to set up our environment. Let's install and import Apache Beam by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5cPHukaOgDDM",
        "outputId": "04826cd7-535c-4331-a22e-fb39411d6da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Remember: You can press shift+enter to run this cell\n",
        "!pip install --quiet apache-beam\n",
        "import apache_beam as beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "30l8_MD-undP"
      },
      "outputs": [],
      "source": [
        "# Set the logging level to reduce verbose information\n",
        "import logging\n",
        "\n",
        "logging.root.setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyoo1gLKtZmU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N29KJTfdMCtB"
      },
      "source": [
        "# What is Apache Beam?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2evhzEuMu8a"
      },
      "source": [
        "Apache Beam is a library for data processing. It is often used for [Extract-Transform-Load (ETL)](https://en.wikipedia.org/wiki/Extract,_transform,_load) jobs, where we:\n",
        "1. *Extract* from a data source\n",
        "2. *Transform* that data\n",
        "3. *Load* that data into a data sink (like a database)\n",
        "\n",
        "Apache Beam makes these jobs easy with the ability to process everything at the same time and its unified model and open-source SDKs. There are many more parts of Beam, but throughout these tutorials, we will break down each part to show you how they will all fit together.\n",
        "\n",
        "For this tutorial, you will use these Beam SDKs to build your own `Pipeline` to process your data.\n",
        "\n",
        "Below, we will run through creating the heart of the `Pipeline`. There are three main abstractions in Beam:\n",
        "1. `Pipeline`\n",
        "2. `PCollection`\n",
        "3. `PTransform`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WecDJbfqpWb1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FditMJAIAp9Q"
      },
      "source": [
        "# 1. Design Your Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYj6e3Hvt1W"
      },
      "source": [
        "## 1.1 What is a Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVJCh9FLBtU9"
      },
      "source": [
        "A `Pipeline` describes the whole cycle of your data processing task, starting from the data sources to the processing transforms you will apply to them until your desired output.\n",
        "\n",
        "`Pipeline` is responsible for reading, processing, and saving the data.\n",
        "Each `PTransform` is done on or outputs a `PCollection`, and this process is done in your `Pipeline`.\n",
        "More glossary details can be found at [here](https://beam.apache.org/documentation/glossary/).\n",
        "\n",
        "A diagram of this process is shown below:\n",
        "\n",
        "<img src='https://beam.apache.org/images/design-your-pipeline-linear.svg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyn8_mB6zN4m"
      },
      "source": [
        "In code, this process will look like this:\n",
        "\n",
        "\n",
        "```\n",
        "# Each `step` represents a specific transform. After `step3`, it will save the data reference to `outputs`.\n",
        "outputs = pipeline | step1 | step2 | step3\n",
        "```\n",
        "\n",
        ">The pipe operator `|`  applies the `PTransform` on the right side of the pipe to the input `PCollection`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUyk2UWypI7g"
      },
      "source": [
        "Pipelines can quickly grow long, so it's sometimes easier to read if we surround them with parentheses and break them into multiple lines.\n",
        "\n",
        "```\n",
        "# This is equivalent to the example above.\n",
        "outputs = (\n",
        "  pipeline\n",
        "  | step1\n",
        "  | step2\n",
        "  | step3\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hmlbjPlvZVY"
      },
      "source": [
        "Sometimes, the transform names aren't very descriptive. Beam allows each transform, or step, to have a unique label, or description. This makes it a lot easier to debug, and it's in general a good practice to start.\n",
        "\n",
        "> You can use the right shift operator `>>` to add a label to your transforms, like `'My description' >> MyTransform`.\n",
        "\n",
        "```\n",
        "# Try to give short but descriptive labels.\n",
        "# These serve both as comments and help debug later on.\n",
        "outputs = (\n",
        "  pipeline\n",
        "  | 'First step' >> step1\n",
        "  | 'Second step' >> step2\n",
        "  | 'Third step' >> step3\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXQ__Kwxvr3J"
      },
      "source": [
        "## 1.2 Loading Our Data\n",
        "\n",
        "Now, you can try to write your own pipeline!\n",
        "\n",
        "First, let's load the example data we will be using throughout this tutorial into our file directory. This [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) consists of a **collection of SMS messages in English tagged as either \"spam\" or \"ham\" (a legitimate SMS\n",
        ")**.\n",
        "\n",
        "For this tutorial, we will create a pipeline to **explore the dataset using Beam to count words in SMS messages that contain spam or ham**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BkZ0wxsBPuvO",
        "outputId": "afac0344-21d2-4c7f-c79f-8f40f08844b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://apachebeamdt/SMSSpamCollection...\n",
            "/ [0 files][    0.0 B/466.7 KiB]                                                \r/ [1 files][466.7 KiB/466.7 KiB]                                                \r\n",
            "Operation completed over 1 objects/466.7 KiB.                                    \n"
          ]
        }
      ],
      "source": [
        "# Creates a data directory with our dataset SMSSpamCollection\n",
        "!mkdir -p data\n",
        "!gsutil cp gs://apachebeamdt/SMSSpamCollection data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiHFDIMnLTbm"
      },
      "source": [
        "**What does the data look like?**\n",
        "\n",
        "This dataset is a `txt` file with 5,574 rows and 4 columns recording the following attributes:\n",
        "1. `Column 1`: The label (either `ham` or `spam`)\n",
        "2. `Column 2`: The SMS as raw text (type `string`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nHQzP9lHJaGn",
        "outputId": "a71f4e53-cc22-4f3f-b91b-6cbccdc5a859",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham\tOk lar... Joking wif u oni...\n",
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "ham\tU dun say so early hor... U c already then say...\n",
            "ham\tNah I don't think he goes to usf, he lives around here though\n",
            "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
            "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
            "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
            "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
          ]
        }
      ],
      "source": [
        "! head data/SMSSpamCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-0WM38KNaTI"
      },
      "source": [
        "## 1.3 Writing Our Own Pipeline\n",
        "\n",
        "Now that we understand our dataset, let's go into creating our pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ajz7-WQCVfj"
      },
      "source": [
        "To initialize a `Pipeline`, you first assign your pipeline `beam.Pipeline()` to a name. Assign your pipeline to the name, `pipeline`, in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cXm0n80E9ZwT"
      },
      "outputs": [],
      "source": [
        "#@title Edit This Code Cell\n",
        "pipeline = beam.Pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q0gzEcos9hjp"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "pipeline = beam.Pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K6tpdVK9KzC"
      },
      "source": [
        "This pipeline will be where we create our transformed `PCollection`. In Beam, your data lives in a `PCollection`, which stands for `Parallel Collection`.\n",
        "\n",
        "A **PCollection** is like a list of elements, but without any order guarantees. This allows Beam to easily parallelize and distribute the `PCollection`'s elements.\n",
        "\n",
        "Now, let's use one of Beam's `Read` transforms to turn our text file (our dataset) into a `PCollection`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS0XWqTiAIlE"
      },
      "source": [
        "## 1.4 Reading from Text File\n",
        "\n",
        "We can use the\n",
        "[`ReadFromText`](https://beam.apache.org/releases/pydoc/current/apache_beam.io.textio.html#apache_beam.io.textio.ReadFromText)\n",
        "transform to read text files into `str` elements.\n",
        "\n",
        "It takes a\n",
        "[_glob pattern_](https://en.wikipedia.org/wiki/Glob_%28programming%29)\n",
        "as an input, and reads all the files that match that pattern. For example, in the pattern `data/*.txt`, the `*` is a wildcard that matches anything. This pattern matches all the files in the `data/` directory with a `.txt` extension. It then **returns one element for each line** in the file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUSzLhhBwdr"
      },
      "source": [
        "Because we only want this pipeline to read the `SMSSpamCollection` file that's in the `data/` directory, we will specify the input pattern to be `'data/SMSSpamCollection'`.\n",
        "\n",
        "We will then use that input pattern with our transform `beam.io.ReadFromText()` and apply it onto our pipeline. The `beam.io.ReadFromText()` transform can take in an input pattern as an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYs5697QEArB"
      },
      "outputs": [],
      "source": [
        "#@title Hint\n",
        "# If you get stuck on the syntax, use the Table of Contents to navigate to 1.1\n",
        "# What is a Pipeline and reread that section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p5z6C65tEmtM",
        "outputId": "055975ae-e80f-444f-e0c7-7ec5612fe2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Edit This Code\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "  pipeline\n",
        "  | 'Padrão' >> beam.io.ReadFromText(inputs_pattern)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zC9blgIXBv5A"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "# Remember: | is the apply function in Beam in Python\n",
        "outputs = (\n",
        "    pipeline\n",
        "      # Remember to add short descriptions to your transforms for good practice and easier understanding\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISm7PF9wE7Ts"
      },
      "source": [
        "## 1.5 Writing to Text File\n",
        "\n",
        "Now, how do we know if we did it correctly? Let's take a look at the text file you just read.\n",
        "\n",
        "You may have noticed that we can't simply `print` the output `PCollection` to see the elements. In Beam, you can __NOT__ access the elements from a `PCollection` directly like a Python list.\n",
        "\n",
        "This is because, depending on the runner,\n",
        "the `PCollection` elements might live in multiple worker machines.\n",
        "\n",
        "However, we can see our output `PCollection` by using a [`WriteToText`](https://beam.apache.org/releases/pydoc/2.27.0/apache_beam.io.textio.html#apache_beam.io.textio.WriteToText) transform to turn our `str` elements into a `txt` file (or another file type of your choosing) and then running a command to show the head of our output file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz73k9d1IUGw"
      },
      "source": [
        "> `beam.io.WriteToText` takes a _file path prefix_ as an input, and it writes the all `str` elements into one or more files with filenames starting with that prefix.\n",
        "> You can optionally pass a `file_name_suffix` as well, usually used for the file extension.\n",
        "> Each element goes into its own line in the output files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D80HrbeKFxCv"
      },
      "source": [
        "Now, you can try it. Save the results to a file path prefix `'output'` and make the file_name_suffix `'.txt'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v6RvVVINKOt9",
        "outputId": "831f2146-2808-4a94-b74f-7b2983899ca8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ansoutput1-00000-of-00001.txt\n",
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham\tOk lar... Joking wif u oni...\n",
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "ham\tU dun say so early hor... U c already then say...\n",
            "ham\tNah I don't think he goes to usf, he lives around here though\n",
            "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
            "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
            "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
            "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
          ]
        }
      ],
      "source": [
        "#@title Edit This Code\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "# Remember: | is the apply function in Beam in Python\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Write results' >> beam.io.WriteToText('output', file_name_suffix = '.txt')\n",
        "      | 'See results' >> beam.io.WriteToText(\"ansoutput1\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print) # or beam.LogElements()\n",
        ")\n",
        "\n",
        "# To run the pipeline\n",
        "pipeline.run()\n",
        "\n",
        "# The command used to view your output txt file.\n",
        "# If you choose to save the file path prefix to a different location or change the file type,\n",
        "# you have to update this command as well.\n",
        "! head output*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nRYxclPaJArp"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      # ADDED\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput1\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "# The file this data is saved to is called \"ansoutput1\" as seen in the WriteToText transform.\n",
        "# The command below and the transform input should match.\n",
        "! head ansoutput1*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iie23Id-O33B"
      },
      "source": [
        "# 2. PTransforms\n",
        "\n",
        "Now that we have read in our code, we can now process our data to explore the text messages that could be classified as spam or non-spam (ham).\n",
        "\n",
        "In order to achieve this, we need to use `PTransforms`.\n",
        "\n",
        "A **`PTransform`** is any data processing operation that performs a processing function on one or more `PCollection`, outputting zero or more `PCollection`.\n",
        "\n",
        "Some PTransforms accept user-defined functions that apply custom logic, which you will learn in the *Advanced Transforms* notebook. The “P” stands for “parallel.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPhZG2GgPCW7"
      },
      "source": [
        "## 2.1 Map\n",
        "\n",
        "One feature to use for the classifier to distinguish spam SMS from ham SMS is to compare the distribution of common words between the two categories. To find the common words for the two categories, we want to perform a frequency count of each word in the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR1YWyvQj1tm"
      },
      "source": [
        "First, because the data set is read line by line, let's clean up the `PCollection` so that the label and the SMS is separated.\n",
        "\n",
        "To do so, we will use the transform `Map`, which takes a **function** and **maps it** to **each element** of the collection and transforms a single input `a` to a single output `b`.\n",
        "\n",
        "In this case, we will use `beam.Map` which takes in a lambda function and uses regex to split the line into a two item list: [label, SMS]. The lambda function we will use is `lambda line: line.split(\"\\t\")`, which splits each element of the `PCollection` by tab and putting them into a list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xey-I5BFvCiH"
      },
      "source": [
        "Add a line of code between the `ReadFromText` and `WriteToText` transform that applies a `beam.Map` transform that takes in the function described above. Remember to add a short description for your transform!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hdrOsitSuj7z",
        "outputId": "75361ff2-1894-4795-d210-7c103bf1047f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output2-00000-of-00001.txt\n",
            "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
            "['ham', 'Ok lar... Joking wif u oni...']\n",
            "['spam', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
            "['ham', 'U dun say so early hor... U c already then say...']\n",
            "['ham', \"Nah I don't think he goes to usf, he lives around here though\"]\n",
            "['spam', \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\"]\n",
            "['ham', 'Even my brother is not like to speak with me. They treat me like aids patent.']\n",
            "['ham', \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\"]\n",
            "['spam', 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.']\n",
            "['spam', 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030']\n"
          ]
        }
      ],
      "source": [
        "#@title Edit This Code\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Tornar lista' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Write results' >> beam.io.WriteToText(\"output2\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head output2*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2k4e3j24wTzM"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      # ADDED\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput2\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput2*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju8TXxBYwvzX"
      },
      "source": [
        "## 2.2 Filter\n",
        "\n",
        "Now that we have a list separating the label and the SMS, let's first focus on only counting words with the **spam** label. In order to process certain elements while igorning others, we want to filter out specific elements in a collection using the transform `Filter`.\n",
        "\n",
        "`beam.Filter` takes in a function that checks a single element a, and returns True to keep the element, or False to discard it.\n",
        "\n",
        "In this case, we want `Filter` to return true if the list contains the label **spam**.\n",
        "\n",
        "We will use a lambda function again for this example, but this time, you will write the lambda function yourself. Add a line of code after your `beam.Map` transform to only return a `PCollection` that only contains lists with the label **spam**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a72v9SQ0zb5u",
        "outputId": "c5198006-cb7d-4365-ab66-f8ccfb08dca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ansoutput3-00000-of-00001.txt\n",
            "['spam', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
            "['spam', \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\"]\n",
            "['spam', 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.']\n",
            "['spam', 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030']\n",
            "['spam', 'SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info']\n",
            "['spam', 'URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18']\n",
            "['spam', 'XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL']\n",
            "['spam', 'England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+']\n",
            "['spam', 'Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged']\n",
            "['spam', '07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow']\n"
          ]
        }
      ],
      "source": [
        "#@title Edit This Code\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Manter apenas o elemento line[0]==spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput3\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput3*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BmvUeOztzCv0"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      # ADDED\n",
        "      | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput3\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput3*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4EcJw45zy6Y"
      },
      "source": [
        "## 2.3 FlatMap\n",
        "\n",
        "Now, that we know we only have SMS labelled spam, we now need to change the element such that instead of each element being a list containing the label and the SMS, each element is a word in the SMS.\n",
        "\n",
        "We can't use `Map`, since `Map` allows us to transform each individual element, but we can't change the number of elements with it.\n",
        "\n",
        "Instead, we want to map a function to each element of a collection. That function returns a list of output elements, so we would get a list of lists of elements. Then we want to flatten the list of lists into a single list.\n",
        "\n",
        "To do this, we will use `FlatMap`, which takes a **function** that transforms a single input `a` into an **iterable of outputs** `b`. But we get a **single collection** containing the outputs of all the elements. In this case, all these elements will be the words found in the SMS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V3w2cz11S_8"
      },
      "source": [
        "Add a `FlatMap` transform that takes in the function `lambda line: re.findall(r\"[a-zA-Z']+\", line[1])` to your code below. The lambda function finds words by finding all elements in the SMS that match the specifications of the regex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "USHHIPO91xDn",
        "outputId": "2f39df35-00cd-4859-b1b1-c20913d600e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ansoutput3-00000-of-00001.txt\n",
            "Free\n",
            "entry\n",
            "in\n",
            "a\n",
            "wkly\n",
            "comp\n",
            "to\n",
            "win\n",
            "FA\n",
            "Cup\n"
          ]
        }
      ],
      "source": [
        "#@title Edit This Code\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput3\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput3*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VFuRMIsN0Edn"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "      # ADDED\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput3\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput3*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFJMyIyJ17lE"
      },
      "source": [
        "## 2.4 Combine\n",
        "\n",
        "Now that each word is one element, we have to count up the elements. To do that we can use [aggregation](https://beam.apache.org/documentation/transforms/python/overview/) transforms, specifically `CombinePerKey` in this instance which transforms an iterable of inputs a, and returns a single output a based on their key.\n",
        "\n",
        "Before using `CombinePerKey` however, we have to associate each word with a numerical value to then combine them.\n",
        "\n",
        "To do this, we add `| 'Pair words with 1' >> beam.Map(lambda word: (word, 1))` to the `Pipeline`, which associates each word with the numerical value 1.\n",
        "\n",
        "With each word assigned to a numerical value, we can now combine these numerical values to sum up all the counts of each word. Like the past transforms, `CombinePerKey` takes in a function and applies it to each element of the `PCollection`.\n",
        "\n",
        "However, instead of writing our own lambda function, we can use pass one of Beam's built-in function `sum` into `CombinePerKey`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Yw_3dfnzI2xA",
        "outputId": "717b1a08-8ce0-4784-f550-f21fea7d9d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ansoutput4-00000-of-00001.txt\n",
            "('Free', 43)\n",
            "('entry', 25)\n",
            "('in', 77)\n",
            "('a', 367)\n",
            "('wkly', 10)\n",
            "('comp', 10)\n",
            "('to', 611)\n",
            "('win', 38)\n",
            "('FA', 4)\n",
            "('Cup', 3)\n"
          ]
        }
      ],
      "source": [
        "#@title Edit This Code\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput4\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput4*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qDyh9_faIkCo"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "outputs = (\n",
        "    pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      # ADDED\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Write results' >> beam.io.WriteToText(\"ansoutput4\", file_name_suffix = \".txt\")\n",
        "      | 'Print the text file name' >> beam.Map(print)\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! head ansoutput4*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9zNI4LMJHur"
      },
      "source": [
        "And we finished! Now that we have a count of all the words to gain better understanding about our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BozCqbzUPItB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv9508k4MeS4"
      },
      "source": [
        "# Full Spam Ham Apache Beam Example\n",
        "\n",
        "Below is a summary of all the code we performed for your convenience. Note you do not need to explicitly call run with the with statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8iWWy3Y7Msxm",
        "outputId": "445acadd-9434-4e06-dca7-80d822298e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://apachebeamdt/SMSSpamCollection...\n",
            "/ [0 files][    0.0 B/466.7 KiB]                                                \r/ [1 files][466.7 KiB/466.7 KiB]                                                \r\n",
            "Operation completed over 1 objects/466.7 KiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet apache-beam\n",
        "!mkdir -p data\n",
        "!gsutil cp gs://apachebeamdt/SMSSpamCollection data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "43oijI4BCAW1",
        "outputId": "b1e65d9a-dbb4-48cd-c065-5e5544a5c733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ham Word Count Head\n",
            "('Go', 10)\n",
            "('until', 21)\n",
            "('jurong', 1)\n",
            "('point', 12)\n",
            "('crazy', 9)\n",
            "('Available', 1)\n",
            "('only', 118)\n",
            "('in', 770)\n",
            "('bugis', 6)\n",
            "('n', 139)\n",
            "Spam Word Count Head\n",
            "('Free', 43)\n",
            "('entry', 25)\n",
            "('in', 77)\n",
            "('a', 367)\n",
            "('wkly', 10)\n",
            "('comp', 10)\n",
            "('to', 611)\n",
            "('win', 38)\n",
            "('FA', 4)\n",
            "('Cup', 3)\n"
          ]
        }
      ],
      "source": [
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'data/SMSSpamCollection'\n",
        "outputs_prefix_ham = 'outputs/fullcodeham'\n",
        "outputs_prefix_spam = 'outputs/fullcodespam'\n",
        "\n",
        "# Ham Word Count\n",
        "with beam.Pipeline() as pipeline:\n",
        "     ham = (\n",
        "      pipeline\n",
        "      | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "      | 'Keep only ham' >> beam.Filter(lambda line: line[0] == \"ham\")\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Format results' >> beam.Map(lambda word_c: str(word_c))\n",
        "      | 'Write results' >> beam.io.WriteToText(outputs_prefix_ham, file_name_suffix = \".txt\")\n",
        "    )\n",
        "\n",
        "# Spam Word Count\n",
        "with beam.Pipeline() as pipeline1:\n",
        "  spam = (\n",
        "    pipeline1\n",
        "    | 'Take in Dataset' >> beam.io.ReadFromText(inputs_pattern)\n",
        "    | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
        "    | 'Filter out only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
        "    | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
        "    | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "    | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "    | 'Format results' >> beam.Map(lambda word_c: str(word_c))\n",
        "    | 'Write results' >> beam.io.WriteToText(outputs_prefix_spam, file_name_suffix = \".txt\")\n",
        "    )\n",
        "\n",
        "print('Ham Word Count Head')\n",
        "! head outputs/fullcodeham*.txt\n",
        "\n",
        "print('Spam Word Count Head')\n",
        "! head outputs/fullcodespam*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNg53x4fyXOO"
      },
      "source": [
        "**One more thing: you can also visualize the Beam pipelines!**\n",
        "\n",
        "Check [this example](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/examples/Interactive%20Beam%20Example.ipynb) to learn more about the interactive Beam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k5cYnOElwrm7",
        "outputId": "d22236a9-a15a-4343-beb9-df728252eb1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
              "            <div id=\"progress_indicator_6a089564a181035c9835b0ce22842c57\">\n",
              "              <div class=\"spinner-border text-info\" role=\"status\"></div>\n",
              "              <span class=\"text-info\">Processing... show_graph</span>\n",
              "            </div>\n",
              "            "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Generated by graphviz version 2.43.0 (0)\n",
              " -->\n",
              "<!-- Title: G Pages: 1 -->\n",
              "<svg width=\"151pt\" height=\"1141pt\"\n",
              " viewBox=\"0.00 0.00 151.00 1141.30\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1137.3)\">\n",
              "<title>G</title>\n",
              "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-1137.3 147,-1137.3 147,4 -4,4\"/>\n",
              "<!-- [15]: Take in Dataset -->\n",
              "<g id=\"node1\" class=\"node\">\n",
              "<title>[15]: Take in Dataset</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"137,-1133.3 6,-1133.3 6,-1097.3 137,-1097.3 137,-1133.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-1111.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Take in Dataset</text>\n",
              "</g>\n",
              "<!-- pcoll318 -->\n",
              "<g id=\"node2\" class=\"node\">\n",
              "<title>pcoll318</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-1043.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Take in Dataset&#45;&gt;pcoll318 -->\n",
              "<g id=\"edge1\" class=\"edge\">\n",
              "<title>[15]: Take in Dataset&#45;&gt;pcoll318</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-1096.99C71.5,-1089.28 71.5,-1080.01 71.5,-1071.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-1071.4 71.5,-1061.4 68,-1071.4 75,-1071.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Separate to list -->\n",
              "<g id=\"node3\" class=\"node\">\n",
              "<title>[15]: Separate to list</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"135.5,-989.3 7.5,-989.3 7.5,-953.3 135.5,-953.3 135.5,-989.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-967.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Separate to list</text>\n",
              "</g>\n",
              "<!-- pcoll318&#45;&gt;[15]: Separate to list -->\n",
              "<g id=\"edge2\" class=\"edge\">\n",
              "<title>pcoll318&#45;&gt;[15]: Separate to list</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-1024.99C71.5,-1017.28 71.5,-1008.01 71.5,-999.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-999.4 71.5,-989.4 68,-999.4 75,-999.4\"/>\n",
              "</g>\n",
              "<!-- pcoll6809 -->\n",
              "<g id=\"node4\" class=\"node\">\n",
              "<title>pcoll6809</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-899.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Separate to list&#45;&gt;pcoll6809 -->\n",
              "<g id=\"edge3\" class=\"edge\">\n",
              "<title>[15]: Separate to list&#45;&gt;pcoll6809</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-952.99C71.5,-945.28 71.5,-936.01 71.5,-927.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-927.4 71.5,-917.4 68,-927.4 75,-927.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Keep only ham -->\n",
              "<g id=\"node5\" class=\"node\">\n",
              "<title>[15]: Keep only ham</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"137,-845.3 6,-845.3 6,-809.3 137,-809.3 137,-845.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-823.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Keep only ham</text>\n",
              "</g>\n",
              "<!-- pcoll6809&#45;&gt;[15]: Keep only ham -->\n",
              "<g id=\"edge4\" class=\"edge\">\n",
              "<title>pcoll6809&#45;&gt;[15]: Keep only ham</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-880.99C71.5,-873.28 71.5,-864.01 71.5,-855.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-855.4 71.5,-845.4 68,-855.4 75,-855.4\"/>\n",
              "</g>\n",
              "<!-- pcoll956 -->\n",
              "<g id=\"node6\" class=\"node\">\n",
              "<title>pcoll956</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-755.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Keep only ham&#45;&gt;pcoll956 -->\n",
              "<g id=\"edge5\" class=\"edge\">\n",
              "<title>[15]: Keep only ham&#45;&gt;pcoll956</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-808.99C71.5,-801.28 71.5,-792.01 71.5,-783.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-783.4 71.5,-773.4 68,-783.4 75,-783.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Find words -->\n",
              "<g id=\"node7\" class=\"node\">\n",
              "<title>[15]: Find words</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"125.5,-701.3 17.5,-701.3 17.5,-665.3 125.5,-665.3 125.5,-701.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-679.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Find words</text>\n",
              "</g>\n",
              "<!-- pcoll956&#45;&gt;[15]: Find words -->\n",
              "<g id=\"edge6\" class=\"edge\">\n",
              "<title>pcoll956&#45;&gt;[15]: Find words</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-736.99C71.5,-729.28 71.5,-720.01 71.5,-711.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-711.4 71.5,-701.4 68,-711.4 75,-711.4\"/>\n",
              "</g>\n",
              "<!-- pcoll5744 -->\n",
              "<g id=\"node8\" class=\"node\">\n",
              "<title>pcoll5744</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-611.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Find words&#45;&gt;pcoll5744 -->\n",
              "<g id=\"edge7\" class=\"edge\">\n",
              "<title>[15]: Find words&#45;&gt;pcoll5744</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-664.99C71.5,-657.28 71.5,-648.01 71.5,-639.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-639.4 71.5,-629.4 68,-639.4 75,-639.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Pair words with 1 -->\n",
              "<g id=\"node9\" class=\"node\">\n",
              "<title>[15]: Pair words with 1</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"143,-557.3 0,-557.3 0,-521.3 143,-521.3 143,-557.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-535.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Pair words with 1</text>\n",
              "</g>\n",
              "<!-- pcoll5744&#45;&gt;[15]: Pair words with 1 -->\n",
              "<g id=\"edge8\" class=\"edge\">\n",
              "<title>pcoll5744&#45;&gt;[15]: Pair words with 1</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-592.99C71.5,-585.28 71.5,-576.01 71.5,-567.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-567.4 71.5,-557.4 68,-567.4 75,-567.4\"/>\n",
              "</g>\n",
              "<!-- pcoll854 -->\n",
              "<g id=\"node10\" class=\"node\">\n",
              "<title>pcoll854</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-467.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Pair words with 1&#45;&gt;pcoll854 -->\n",
              "<g id=\"edge9\" class=\"edge\">\n",
              "<title>[15]: Pair words with 1&#45;&gt;pcoll854</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-520.99C71.5,-513.28 71.5,-504.01 71.5,-495.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-495.4 71.5,-485.4 68,-495.4 75,-495.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Group and sum -->\n",
              "<g id=\"node11\" class=\"node\">\n",
              "<title>[15]: Group and sum</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"137,-413.3 6,-413.3 6,-377.3 137,-377.3 137,-413.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-391.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Group and sum</text>\n",
              "</g>\n",
              "<!-- pcoll854&#45;&gt;[15]: Group and sum -->\n",
              "<g id=\"edge10\" class=\"edge\">\n",
              "<title>pcoll854&#45;&gt;[15]: Group and sum</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-448.99C71.5,-441.28 71.5,-432.01 71.5,-423.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-423.4 71.5,-413.4 68,-423.4 75,-423.4\"/>\n",
              "</g>\n",
              "<!-- pcoll8195 -->\n",
              "<g id=\"node12\" class=\"node\">\n",
              "<title>pcoll8195</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-323.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Group and sum&#45;&gt;pcoll8195 -->\n",
              "<g id=\"edge11\" class=\"edge\">\n",
              "<title>[15]: Group and sum&#45;&gt;pcoll8195</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-376.99C71.5,-369.28 71.5,-360.01 71.5,-351.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-351.4 71.5,-341.4 68,-351.4 75,-351.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Format results -->\n",
              "<g id=\"node13\" class=\"node\">\n",
              "<title>[15]: Format results</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"134,-269.3 9,-269.3 9,-233.3 134,-233.3 134,-269.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-247.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Format results</text>\n",
              "</g>\n",
              "<!-- pcoll8195&#45;&gt;[15]: Format results -->\n",
              "<g id=\"edge12\" class=\"edge\">\n",
              "<title>pcoll8195&#45;&gt;[15]: Format results</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-304.99C71.5,-297.28 71.5,-288.01 71.5,-279.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-279.4 71.5,-269.4 68,-279.4 75,-279.4\"/>\n",
              "</g>\n",
              "<!-- pcoll4276 -->\n",
              "<g id=\"node14\" class=\"node\">\n",
              "<title>pcoll4276</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-179.3\" rx=\"18\" ry=\"18\"/>\n",
              "</g>\n",
              "<!-- [15]: Format results&#45;&gt;pcoll4276 -->\n",
              "<g id=\"edge13\" class=\"edge\">\n",
              "<title>[15]: Format results&#45;&gt;pcoll4276</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-232.99C71.5,-225.28 71.5,-216.01 71.5,-207.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-207.4 71.5,-197.4 68,-207.4 75,-207.4\"/>\n",
              "</g>\n",
              "<!-- [15]: Write results -->\n",
              "<g id=\"node15\" class=\"node\">\n",
              "<title>[15]: Write results</title>\n",
              "<polygon fill=\"none\" stroke=\"blue\" points=\"129.5,-125.3 13.5,-125.3 13.5,-89.3 129.5,-89.3 129.5,-125.3\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-103.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">[15]: Write results</text>\n",
              "</g>\n",
              "<!-- pcoll4276&#45;&gt;[15]: Write results -->\n",
              "<g id=\"edge14\" class=\"edge\">\n",
              "<title>pcoll4276&#45;&gt;[15]: Write results</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-160.99C71.5,-153.28 71.5,-144.01 71.5,-135.41\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-135.4 71.5,-125.4 68,-135.4 75,-135.4\"/>\n",
              "</g>\n",
              "<!-- ham -->\n",
              "<g id=\"node16\" class=\"node\">\n",
              "<title>ham</title>\n",
              "<ellipse fill=\"none\" stroke=\"blue\" cx=\"71.5\" cy=\"-26.65\" rx=\"26.8\" ry=\"26.8\"/>\n",
              "<text text-anchor=\"middle\" x=\"71.5\" y=\"-22.95\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"blue\">ham</text>\n",
              "</g>\n",
              "<!-- [15]: Write results&#45;&gt;ham -->\n",
              "<g id=\"edge15\" class=\"edge\">\n",
              "<title>[15]: Write results&#45;&gt;ham</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M71.5,-89.23C71.5,-81.7 71.5,-72.54 71.5,-63.61\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"75,-63.35 71.5,-53.35 68,-63.35 75,-63.35\"/>\n",
              "</g>\n",
              "</g>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "            $(\"#progress_indicator_6a089564a181035c9835b0ce22842c57\").remove();\n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "            $(\"#progress_indicator_6a089564a181035c9835b0ce22842c57\").remove();\n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "ib.show_graph(pipeline)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "aab5fceeb08468f7e142944162550e82df74df803ff2eb1987d9526d4285522f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}